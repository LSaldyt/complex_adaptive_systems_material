%% Template for a preprint Letter or Article for submission
%% to the journal Nature.
%% Written by Peter Czoschke, 26 February 2004
%% Edited by Lucas Saldyt, August 2018

\documentclass{nature}

%% make sure you have the nature.cls and naturemag.bst files where
%% LaTeX can find them...?
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[nottoc]{tocbibind}

\bibliographystyle{naturemag}

\title{Distributed Complex Adaptive Behavior in Cognitive Architectures}

%% Notice placement of commas and superscripts and use of &
%% in the author list

\author{Lucas Saldyt}


\begin{document}

\maketitle

\begin{affiliations}
    \item Affiliation: Arizona State University (lsaldyt@asu.edu)
\end{affiliations}

\begin{abstract}
    Modern cognitive architectures display interesting architectural properties similar to those showed in many distributed systems.
    Distributed nature is a fundamental necessity, given that the human brain is decentralized and displays adaptive properties.
    In other words, any cognitive architecture that is meant to model what people do must display the same abstract architectural properties that humans do.
    We build on work done with the copycat cognitive architecture, where it was shown that the emulation of distribution did not fully satisfy expectations.
    This paper will compare the graph-theory metrics from the LIDA cognitive architecture to graph-theory metrics in other distributed systems, like social networks or true neural networks, and attempt to build or derive the pure mathematical model behind these processes.

%% For Nature, the abstract is really an introductory paragraph set
%% in bold type.  This paragraph must be ``fully referenced'' and
%% less than 180 words for Letters.  This is the thing that is
%% supposed to be aimed at people from other disciplines and is
%% arguably the most important part to getting your paper past the
%% editors.  End this paragraph with a sentence like ``Here we
%% show...'' or something similar.
\end{abstract}

%%Then the body of the main text appears after the intro paragraph.
%%Figure environments can be left in place in the document.
%%\verb|\includegraphics| commands are ignored since Nature wants
%%the figures sent as separate files and the captions are
%%automatically moved to the end of the document (they are printed
%%out with the \verb|\end{document}| command. However, tables must
%%be manually moved to the end of the document, after the addendum.
%%
%%Citation of Einstein's paper \cite{Einstein}.

\section{Introduction}
    The entire physical world is fundamentally decentralized. 
    Each element in the physical world (i.e. each agent in a complex adaptive system) computes itself, independently of each other element, and only ever interacts with agents that are near it in space. 
    Of particular interest is human and animal brains, where each neuron acts as a free computational element, with its own memory and processing power.
    However, a generic modern computer model (with a Von-Neumann architecture) will pass all computation through its central processing unit and a central memory storage. 
    In principle, it is possible to emulate a distributed system on a faster centralized system, however, this architectural feature may hinder progress (On a computer architecture with as many independent nodes as agents in the simulation, an fully-parallelizable algorithm that previously ran on the order of the number of agents can effectively run in constant time). 
    Since the distribution and adaptability of the human brain is such a salient abstract architectural feature, special care must be taken in AI systems to emulate this (It should be considered as strong observation evidence to replicate).
    This project will investigate modern distributed artificial intelligence systems and mathematical models of memory. 
    Of particular interest is Kanerva's original mathematical model of memory, and Marcelo [Brogliato?]'s parallel sparse distributed memory framework. 
    And now, the most advanced cognitive architecture is the LIDA architecture. 
    An interesting consequence of these models is that they should not contain centralized or serialized portions, as these will vastly limit their capability for parallel work. 
    This project will analyze the connectivity of existing models in LIDA: The extend to which they are distributed, how, where, and to what degree clustering occurs, and the transitions and equilibrium points that the model traverses or ends on.
    The expectation is that the LIDA architecture should show high degrees of distribution, and network metrics comparable to those in "ground truth" systems. 
    If these expectations are not met, then the cause of this will be isolated and perturbed.
    Fundamentally, we wish to know the degree to which LIDA is distributed, and how this degree affects performance on cognitively interesting classes.

\section{Importance}

    Creating a better understanding of memory in the brain will allow the creation of legitimate cognitive architectures. 
    The creation of a theoretical model will lead to possible perturbations that can be attempted on an actual physical system.
    Also, a distributed cognitive architecture acts, in principle, much like advanced pieces of software, such as an operating system or a virtual machine.
    Therefore, insights about the nature of a distributed cognitive system should be telling about other distributed systems. 
    And the notion of a distributed system is so general that these implications will be wide and important.
    Another notable example is, of course, distributed social systems, where agents are people.

\section{History}

    %% Pre-kanerva?
    %% Kanerva
    %% Koch
    %% Indiana U networks
    %% Hofstadter
    %% Brogliato and Linhares
    %% LIDA architecture documents

    The history of cognitive architectures is long and wide, but we will focus on where it is currently: 
    The LIDA architecture essentially takes the most salient notions from psychology and applies them to a computer program, which the hopes of creating a biologically and psychologically plausible machine intelligence.
    Generally, LIDA has a closed loop cycle containing, for example, perception, memory, processing, and action.
    Actions and perceptions are stochastic, and the combination of several pieces (several agents) results in emergence of a coherent whole.

\section{Work}

    Essentially, my work will be to analyze LIDA in the same way that [Indiana U paper] analyzed brain networks and [Small world paper] analyzed social (etc) networks. 
    Additionally, I will derive the mathematics describing copycat, or a LIDA implementation of copycat (Specifically, I'd like to look at the formula that control the "movement" of agents within the system -- for example, the "elegance" metric, which decides when to stop processing and decide on an answer). 

    This analysis will rely on a series of metrics, including:
    \begin{enumerate}
        \item common network metrics, like length and clustering coefficient.
        \item $\chi ^2$ tests when formulas are varied (see copycat paper)
        \item runtime, parallelizability factors (ahmdals)
        \item equilibrium points (see metacat, and the looping point caused by the "xyd" problem as well)
    \end{enumerate}


\section{Tasks}

Tasks:
 - Citations
 - Slides about graph theory, essentially
 - Introduction, Importance, History, Future work, Tasks, Outline

\section{Outline}


%% []. Any simulation of these processes must also display distributed properties. For example, it should be adaptive (i.e. Plasticity). However, many existing models do not display these properties. Every computer that exists today is based on the Von Neumann architecture. However, Neumann himself delivered a series of lectures discussing the fundamental differences between this classical architecture and the "architecture" of the human (or any) brain. Largely, the underlying
%% substrate actually has a large effect on the way the models are developed. In principle, it is possible to emulate the brain on a computer, however, some architectural differences are more fundamental then others. I will argue that supercomputing, quantum computing, or neuromorphic architectures are better suited for artificial intelligence: Not only can they speed up classical algorithms, they can be used as the basis for algorithms that better mimic the function of the brain.
%% 
%% [Another potentially (marginally) interesting example of this success is the application of GPUs to neural net architectures.]
%% 
%% There are a few fundamental reasons behind this:
%% If there are many processors as agents (neurons), then the runtime of the simulation can be decreased to the time of the longest serial element. So, for example, a supercomputer with 10^11 nodes could allocate one neuron per node for a human brain. A more feasible case is, for example, the C-elegans connection (302 neurons). 
%% 
%% However, even a single neuron is extremely complicated to simulate. For example, []. The real question is: Is there an existing symbolic level below which the simulation need not be quantum-accurate? For example, can the electrochemistry of individual neurons be approximated classically, or does true simulation require a quantum computer? It seem unlikely that neurons depend on quantum computation, given the decoherence time and inherent noise of the environment, and it
%% may also be true that an abstract model of neurochemistry gives results within statistical error of quantum-accurate results. However, this is the lowest level of detail. Alternatively, we might consider the highest level, which is the "grand" architectural structures of the brain. So, we might guess that distribution and cooperation are in fact exploiting an interesting property of the universe: namely that computation is distributed and parallel. This is but one of
%% many architectural differences (see Von Neumann), but is certainly an important one, especially because of the speedup (amhdahls). 
%% With this in mind, we consider the distributed nature of blackboard architectures, and artificial intelligence systems in general: what perturbations can we make to the brain that result in similar effects in simulation? Any simulation of intelligence must chose a level of abstraction to operate at, and assert that this level of abstraction is complete, and then evaluate and test at that level.
%% 
%% Optionally, we may start with the assertion that any quantum computer is capable of simulating the brain. This is inherently true, because a quantum computer can simulate anything (supposing, of course, quantum mechanics is correct -- which many seem to agree it is)
%% However, this level of detail may not be needed. So, the next level of abstraction is, roughly, electrochemistry. Perhaps an idealized model of the brain accounts for what we desire? 
%% We can move along these levels of abstractions, performing tests until the desired behavior is gone. Then, we will know with certainty that the last level of abstraction is responsible for what we call "intelligence".
%% 
%% Notice, of course, that a naive (and wrong) assumption that each level of abstraction is equally likely would mean that the higher levels of abstraction should actually be addressed first, because it is more likely that the brain depends on them. My personal vote is actually on the electrochemical level, but I'm no expert... anyway..
%% 
%% So, considering all of these levels, it somewhat interesting to note that the brain isn't actualy *just* a nonlinear system. It's actually a non-linear system where each agent is a non-linear sub-system. Generalizing, it's a recursive non-linear composite.... weird, no?
%% We might consider how one would reason about a system of non-linear systems in the abstract..
%% Inverse occam's razor: If we've only observed a portion of a very-complicated system's behavior, we should posit complicated means, because any simplified approach will likely oversimplify (this is just occam's razor, but applied strictly) [Minsky].

%% \begin{figure}
%% %%%\includegraphics{something} % this command will be ignored
%% \caption{Each figure legend should begin with a brief title for
%% the whole figure and continue with a short description of each
%% panel and the symbols used. For contributions with methods
%% sections, legends should not contain any details of methods, or
%% exceed 100 words (fewer than 500 words in total for the whole
%% paper). In contributions without methods sections, legends should
%% be fewer than 300 words (800 words or fewer in total for the whole
%% paper).}
%% \end{figure}
%% 
%% \section*{Another Section}
%% 
%% Sections can only be used in Articles.  Contributions should be
%% organized in the sequence: title, text, methods, references,
%% Supplementary Information line (if any), acknowledgements,
%% interest declaration, corresponding author line, tables, figure
%% legends.
%% 
%% Spelling must be British English (Oxford English Dictionary)
%% 
%% In addition, a cover letter needs to be written with the
%% following:
%% \begin{enumerate}
%%  \item A 100 word or less summary indicating on scientific grounds
%% why the paper should be considered for a wide-ranging journal like
%% \textsl{Nature} instead of a more narrowly focussed journal.
%%  \item A 100 word or less summary aimed at a non-scientific audience,
%% written at the level of a national newspaper.  It may be used for
%% \textsl{Nature}'s press release or other general publicity.
%%  \item The cover letter should state clearly what is included as the
%% submission, including number of figures, supporting manuscripts
%% and any Supplementary Information (specifying number of items and
%% format).
%%  \item The cover letter should also state the number of
%% words of text in the paper; the number of figures and parts of
%% figures (for example, 4 figures, comprising 16 separate panels in
%% total); a rough estimate of the desired final size of figures in
%% terms of number of pages; and a full current postal address,
%% telephone and fax numbers, and current e-mail address.
%% \end{enumerate}
%% 
%% See \textsl{Nature}'s website
%% (\texttt{http://www.nature.com/nature/submit/gta/index.html}) for
%% complete submission guidelines.
%% 
%% \begin{methods}
%% Put methods in here.  If you are going to subsection it, use
%% \verb|\subsection| commands.  Methods section should be less than
%% 800 words and if it is less than 200 words, it can be incorporated
%% into the main text.
%% 
%% \subsection{Method subsection.}
%% 
%% Here is a description of a specific method used.  Note that the
%% subsection heading ends with a full stop (period) and that the
%% command is \verb|\subsection{}| not \verb|\subsection*{}|.
%% 
%% \end{methods}
%% 
%% %% Put the bibliography here, most people will use BiBTeX in
%% %% which case the environment below should be replaced with
%% %% the \bibliography{} command.
%% 
%% % \begin{thebibliography}{1}
%% % \bibitem{dummy} Articles are restricted to 50 references, Letters
%% % to 30.
%% % \bibitem{dummyb} No compound references -- only one source per
%% % reference.
%% % \end{thebibliography}
%% 
%% \bibliographystyle{naturemag}
%% \bibliography{sample}
%% 
%% 
%% %% Here is the endmatter stuff: Supplementary Info, etc.
%% %% Use \item's to separate, default label is "Acknowledgements"
%% 
%% \begin{addendum}
%%  \item Put acknowledgements here.
%%  \item[Competing Interests] The authors declare that they have no
%% competing financial interests.
%%  \item[Correspondence] Correspondence and requests for materials
%% should be addressed to A.B.C.~(email: myaddress@nowhere.edu).
%% \end{addendum}
%% 
%% %%
%% %% TABLES
%% %%
%% %% If there are any tables, put them here.
%% %%
%% 
%% \begin{table}
%% \centering
%% \caption{This is a table with scientific results.}
%% \medskip
%% \begin{tabular}{ccccc}
%% \hline
%% 1 & 2 & 3 & 4 & 5\\
%% \hline
%% aaa & bbb & ccc & ddd & eee\\
%% aaaa & bbbb & cccc & dddd & eeee\\
%% aaaaa & bbbbb & ccccc & ddddd & eeeee\\
%% aaaaaa & bbbbbb & cccccc & dddddd & eeeeee\\
%% 1.000 & 2.000 & 3.000 & 4.000 & 5.000\\
%% \hline
%% \end{tabular}
%% \end{table}

%% Sources and discussion:

This is a citation:
\cite{einstein}

\bibliography{sources}

\end{document}
